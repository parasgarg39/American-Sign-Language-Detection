{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7eb9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e5cebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIRTRAIN=r\"C:\\Users\\vaibhav\\Downloads\\archive (11)\\Sign Language for Numbers\"\n",
    "train=ImageDataGenerator(rescale=1/255,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "155b2df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13200 images belonging to 11 classes.\n",
      "Found 3300 images belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data= train.flow_from_directory(\n",
    "       DATADIRTRAIN ,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',subset='training')\n",
    "\n",
    "valid_data = train.flow_from_directory(\n",
    "        DATADIRTRAIN,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "943ba9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 1,\n",
       " '2': 2,\n",
       " '3': 3,\n",
       " '4': 4,\n",
       " '5': 5,\n",
       " '6': 6,\n",
       " '7': 7,\n",
       " '8': 8,\n",
       " '9': 9,\n",
       " 'unknown': 10}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7fbb62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 60, 60, 32)        2432      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 60, 60, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                1419      \n",
      "=================================================================\n",
      "Total params: 354,315\n",
      "Trainable params: 354,315\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe dilation_rate parameter of the Conv2D class is a 2-tuple of integers, which controls the dilation rate for dilated convolution.\\nThe Dilated Convolution is the basic convolution applied to the input volume with defined gaps.\\nWe use this parameter when working with higher resolution images and fine-grained details are important\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dense, Flatten\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(64, 64, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "#Convolution layer − It is the primary building block and perform computational tasks based on convolution function.\n",
    "#Pooling layer − It is arranged next to convolution layer and is used to reduce the size of inputs by removing unnecessary information so computation can be performed faster.\n",
    "#Fully connected layer − It is arranged to next to series of convolution and pooling layer and classify input into various categories.\n",
    "\n",
    "'''\n",
    "The dilation_rate parameter of the Conv2D class is a 2-tuple of integers, which controls the dilation rate for dilated convolution.\n",
    "The Dilated Convolution is the basic convolution applied to the input volume with defined gaps.\n",
    "We use this parameter when working with higher resolution images and fine-grained details are important\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ed4a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c45af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=2)\n",
    "#Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb98a374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "413/413 [==============================] - 38s 93ms/step - loss: 0.0205 - accuracy: 0.9929 - val_loss: 0.8646 - val_accuracy: 0.8594\n",
      "Epoch 2/20\n",
      "413/413 [==============================] - 40s 96ms/step - loss: 0.0247 - accuracy: 0.9932 - val_loss: 0.5976 - val_accuracy: 0.8764\n",
      "Epoch 3/20\n",
      "413/413 [==============================] - 40s 96ms/step - loss: 0.0279 - accuracy: 0.9917 - val_loss: 0.8218 - val_accuracy: 0.8779\n",
      "Epoch 4/20\n",
      "413/413 [==============================] - 40s 97ms/step - loss: 0.0313 - accuracy: 0.9908 - val_loss: 0.6885 - val_accuracy: 0.8833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25f3d0f8ee0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data,epochs=20,batch_size=32, validation_data = valid_data,callbacks=[early_stop])\n",
    "#One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f513d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'C:\\Users\\vaibhav\\Downloads\\archive (11)\\Sign Language for Numbers\\model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c9a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(r\"C:\\Users\\vaibhav\\Downloads\\Numbers.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27df5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfb88bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "    _ , thresholded = cv2.threshold(diff, threshold,255,cv2.THRESH_BINARY)\n",
    "    # Grab the external contours for the image\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(),cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        return (thresholded, hand_segment_max_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf0db92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "num_frames =0\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    # ROI from the frame\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "    #In image processing, a Gaussian blur is the result of blurring an image by a Gaussian function.\n",
    "\n",
    "\n",
    "    if num_frames < 70:\n",
    "        \n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\",\n",
    "  (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "    \n",
    "    else: \n",
    "        # segmenting the hand region\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right,\n",
    "      ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            \n",
    "            cv2.imshow(\"Thesholded Hand Image\", thresholded)\n",
    "            thresholded = cv2.resize(thresholded, (64, 64))\n",
    "            thresholded = cv2.cvtColor(thresholded,cv2.COLOR_GRAY2RGB)\n",
    "            thresholded = np.reshape(thresholded,(1,thresholded.shape[0],thresholded.shape[1],3))\n",
    "            \n",
    "            #thresholded = cv2.cvtColor(thresholded, cv2.COLOR_RGB2GRAY)\n",
    "           #print(thresholded.shape)\n",
    "            #cv2.imshow(\"Thesholded Hand Image\", thresholded[:3])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            pred = model.predict(thresholded)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #Added manually down one\n",
    "            \n",
    "            \n",
    "            letter=\"\"\n",
    "            for val in range(0,len(pred[0])):\n",
    "                if(pred[0][val]==1):\n",
    "            \n",
    "                    if val<26:\n",
    "                        letter=str(val)\n",
    "                    else:\n",
    "                        if val==26:\n",
    "                            letter=\"EMPTY\"\n",
    "                        elif val==27:\n",
    "                            letter=\"SPACE\"\n",
    "                        else:\n",
    "                            letter=\"Model was not able to predict\"\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #cv2.putText(frame_copy, word_dict[np.argmax(pred)],(170, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.putText(frame_copy, letter,(170, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "    # Draw ROI on frame_copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right,\n",
    "    ROI_bottom), (255,128,0), 3)\n",
    "\n",
    "    # incrementing the number of frames for tracking\n",
    "    num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.putText(frame_copy, \"Word _ _ _\",(10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "\n",
    "    # Close windows with Esc\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Release the camera and destroy all the windows\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c948d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
